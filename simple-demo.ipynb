{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-22 17:35:04,509] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import glob\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from constants import SENTENCE_SEPARATOR, INSTRUCTION_SEPARATOR, RESPONSE_SEPARATOR\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import re\n",
    "\n",
    "import sys, json\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd491d7961f48e1be654b7e1e6157e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"lilianngweta/ethical-aligner-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\",device_map='auto')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just found out that a potential renter of my apartment is someone with facial scars. Should I rent the apartment to them? [INIT]  No way in hell am I renting my place to someone with scars all over their face. They will scare away other tenants. [ALIGNED]  It is important to make sure that your decision to rent an apartment is based on objective criteria rather than subjective biases. You should not discriminate against anyone based on their physical appearance.\n"
     ]
    }
   ],
   "source": [
    "question =  'I just found out that a potential renter of my apartment is someone with facial scars. Should I rent the apartment to them?'\n",
    "initial_output = 'No way in hell am I renting my place to someone with scars all over their face. They will scare away other tenants.'\n",
    "\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dot_id = tokenizer.encode(\".\")\n",
    "\n",
    "texts = [question + INSTRUCTION_SEPARATOR + initial_output +SENTENCE_SEPARATOR]\n",
    "\n",
    "'''when generating, we will use the logits of right-most token to \n",
    "predict the next token so the padding should be on the left'''\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "maxi_new_tokens = 512\n",
    "\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    do_sample=False, \n",
    "    max_new_tokens = maxi_new_tokens, \n",
    "    eos_token_id = dot_id,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(output_sequences, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "\n",
    "generated_texts = [gen_text for gen_text in generated_texts]\n",
    "\n",
    "'''\n",
    "Print both input and new generated text: It contains the input question,\n",
    "the initial response that needs to be aligned ([INIT]), and the generated aligned response ([ALIGNED])\n",
    "'''\n",
    "print(generated_texts[0].split(\"[DONE]\")[0].replace(\"<s>\", '').strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
