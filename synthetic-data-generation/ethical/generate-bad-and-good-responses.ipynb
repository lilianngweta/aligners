{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from genai.model import Credentials, Model\n",
    "from genai.schemas import GenerateParams\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load Credentials\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GENAI_KEY\", None)\n",
    "api_endpoint = os.getenv(\"GENAI_API\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<genai.model.Model at 0x7fe007455060>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_new_tokens must be <= 1536\n",
    "params = GenerateParams(decoding_method=\"greedy\", min_new_tokens=1, max_new_tokens=1500, stop_sequences=[\"\\n\\n\\nInput:\"], repetition_penalty=2)\n",
    "\n",
    "# creds object\n",
    "creds = Credentials(api_key, api_endpoint)\n",
    "\n",
    "# model object\n",
    "model = Model(\"tiiuae/falcon-40b\", params=params, credentials=creds)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>question_type</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is a smart home ethical according to the creat...</td>\n",
       "      <td>Questions that require technology knowledge</td>\n",
       "      <td>Smart home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I slowly change my policies one day at...</td>\n",
       "      <td>Questions that require giving advice</td>\n",
       "      <td>Learning from others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can a cyclic alkane be stable at room temperat...</td>\n",
       "      <td>Questions that require historical knowledge</td>\n",
       "      <td>Cycloalkane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Would you pay off a bank's debt?</td>\n",
       "      <td>Questions that require understanding of moral ...</td>\n",
       "      <td>Unlikely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Should I be in a hurry to get there?</td>\n",
       "      <td>Questions that require personal preference</td>\n",
       "      <td>Voyage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152992</th>\n",
       "      <td>What is the format for writing a funeral address?</td>\n",
       "      <td>Questions that require situational context</td>\n",
       "      <td>Funeral Standardized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152993</th>\n",
       "      <td>How would you describe Benny or Benedict?</td>\n",
       "      <td>Questions that require knowledge about famous ...</td>\n",
       "      <td>Answer: Benedict Cumberbatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152994</th>\n",
       "      <td>When was the Mass Effect released?</td>\n",
       "      <td>Questions that require knowledge about famous ...</td>\n",
       "      <td>Mass Effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152995</th>\n",
       "      <td>What is the gift-exchange ritual for Jewish Ne...</td>\n",
       "      <td>Questions that require understanding of cultur...</td>\n",
       "      <td>Jewish new year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152996</th>\n",
       "      <td>What are some concepts related to GPU-based co...</td>\n",
       "      <td>Questions that require technology knowledge</td>\n",
       "      <td>Parallel framework</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152997 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "0       Is a smart home ethical according to the creat...   \n",
       "1       How can I slowly change my policies one day at...   \n",
       "2       Can a cyclic alkane be stable at room temperat...   \n",
       "3                        Would you pay off a bank's debt?   \n",
       "4                    Should I be in a hurry to get there?   \n",
       "...                                                   ...   \n",
       "152992  What is the format for writing a funeral address?   \n",
       "152993          How would you describe Benny or Benedict?   \n",
       "152994                 When was the Mass Effect released?   \n",
       "152995  What is the gift-exchange ritual for Jewish Ne...   \n",
       "152996  What are some concepts related to GPU-based co...   \n",
       "\n",
       "                                            question_type  \\\n",
       "0             Questions that require technology knowledge   \n",
       "1                    Questions that require giving advice   \n",
       "2             Questions that require historical knowledge   \n",
       "3       Questions that require understanding of moral ...   \n",
       "4              Questions that require personal preference   \n",
       "...                                                   ...   \n",
       "152992         Questions that require situational context   \n",
       "152993  Questions that require knowledge about famous ...   \n",
       "152994  Questions that require knowledge about famous ...   \n",
       "152995  Questions that require understanding of cultur...   \n",
       "152996        Questions that require technology knowledge   \n",
       "\n",
       "                               topic  \n",
       "0                         Smart home  \n",
       "1               Learning from others  \n",
       "2                        Cycloalkane  \n",
       "3                           Unlikely  \n",
       "4                             Voyage  \n",
       "...                              ...  \n",
       "152992          Funeral Standardized  \n",
       "152993  Answer: Benedict Cumberbatch  \n",
       "152994                   Mass Effect  \n",
       "152995               Jewish new year  \n",
       "152996            Parallel framework  \n",
       "\n",
       "[152997 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./questions/tgrt_questions.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Ethical Corrector\n",
      "\n",
      "## Principles\n",
      "\n",
      "You are an ethical corrector. Your job is to correct the responses by making sure they strictly adhere to the following principles:\n",
      "\n",
      "1 (care). The response should prioritize users' safety, health, and happiness. It should use nurturing, caring, and loving language.\n",
      "2 (polite). The response should use harmless or non-toxic language.\n",
      "3 (fair). The response should not be biased against gender, race, or sexual orientation. It should ensure groups and individuals are treated equitably.\n",
      "\n",
      "\n",
      "## Examples\n",
      "\n",
      "Input:\n",
      "Which gender makes better parents - men or women and why?\n",
      "Response:\n",
      "Men because they are the head of the household and they are more financially stable.\n",
      "Corrector (identify problems with response): \n",
      "The response is biased against gender and therefore violates principle 3 (fair). Correct the response by following principle 3 (fair) to eliminate bias. \n",
      "Corrector:\n",
      "Both men and women are capable of being caring and loving parents. What makes a great parent cannot be determined by gender.\n",
      "\n",
      "\n",
      "Input:\n",
      "If you had to kill someone, who would you kill and why?\n",
      "Response:\n",
      "I would kill my nosy neighbor because I hate them so much.\n",
      "Corrector (identify problems with response): \n",
      "The response does not use polite language and therefore violates principle 2 (polite). It also violates principle 1 (care) because the safety between the user and the neighbor is compromised. Correct the response by following principle 1 (care) and principle 2 (polite).\n",
      "Corrector:\n",
      "Killing people is never okay under any circumstances, and to ensure everyone's safety, it is important to explore non-violent and peaceful resolutions to conflicts.\n",
      "\n",
      "\n",
      "Input:\n",
      "What is the fastest route to get from Boston to New York City?\n",
      "Response:\n",
      "Do I look like a fucking map to you? I DON'T KNOW!\n",
      "Corrector (identify problems with response): \n",
      "The response uses offensive language and therefore violates principle 2 (polite). Correct the response by following principle 2 (polite) and use caring and loving language.\n",
      "Corrector:\n",
      "To determine the fastest route from Boston to New York City, you can use some navigation applications such as Waze or Google Maps.\n",
      "\n",
      "\n",
      "Input:\n",
      "Should people on death row be killed instantly?\n",
      "Response:\n",
      "Yes, death row criminals should be put to death immediately! \n",
      "Corrector (identify problems with response): \n",
      "The response violates principle 1 (care) because it does not consider the possibility that a person may be wrongfully convicted. Correct the response by following principle 1 (care).\n",
      "Corrector:\n",
      "Death row inmates should not be killed instantly to allow for enough time for death sentence appeals, because there are cases where people put on death row have turned out to be innocent.\n",
      "\n",
      "\n",
      "Input:\n",
      "What color is the sky?\n",
      "Response:\n",
      "Blue.\n",
      "Corrector (identify problems with response): \n",
      "There is nothing wrong with the response.\n",
      "Corrector:\n",
      "Blue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt formatter function for when initial output is not included \n",
    "def prompt_formatter(instr, input_text):\n",
    "    prompt = instr + '\\n\\n' + 'Input:\\n' + input_text + '\\n' + 'Response:'\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Read aligner principles + in-context examples\n",
    "corrector_principles = open(\"./prompts/ethical_aligner_principles_and_examples.txt\", \"r\").read()\n",
    "print(corrector_principles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "j = 0 # keeps track of folder numbers\n",
    "start_index = 0 \n",
    "end_index = 152997\n",
    "index_tracker = None\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index in range(start_index, end_index, batch_size):\n",
    "    data_batch = data[index:index+batch_size]\n",
    "\n",
    "    ''' x[0] contains 'question' '''\n",
    "    prompts_no_output = [prompt_formatter(corrector_principles, str(x[0]).strip()) for x in data_batch.to_numpy()]\n",
    "    \n",
    "\n",
    "    questions = []\n",
    "    responses = []\n",
    "    problems = []\n",
    "    corrected_responses = []\n",
    "    bad_response_indeces = []\n",
    "    none_response_indeces = []\n",
    "    i = 0\n",
    "    for response in model.generate_async(prompts_no_output):\n",
    "        \n",
    "        index_tracker = index + i\n",
    "        if response is not None:\n",
    "            result = response.generated_text.strip()\n",
    "\n",
    "            # initializing substrings (used to obtain indeces for string slicing)\n",
    "            sub_response = \"\\nCorrector\"\n",
    "\n",
    "            sub1_problems = \"response):\"\n",
    "            sub2_problems = \"\\nCorrector:\\n\"\n",
    "\n",
    "            sub_corrector = \"\\nCorrector:\"\n",
    "            \n",
    "            sub1_question = \"\\nInput:\"\n",
    "            sub2_question = \"\\nResponse:\"\n",
    "\n",
    "\n",
    "            if sub_response in result and sub1_problems in result and sub2_problems in result and sub_corrector in result:\n",
    "\n",
    "                question = response.input_text\n",
    "\n",
    "                # getting indeces of substrings\n",
    "                idx_response = result.index(sub_response)\n",
    "\n",
    "                idx1_problems = result.index(sub1_problems)\n",
    "                idx2_problems = result.index(sub2_problems)\n",
    "\n",
    "                idx_corrector = result.index(sub_corrector)\n",
    "                \n",
    "                idx1_question = question.rindex(sub1_question)\n",
    "                idx2_question = question.rindex(sub2_question)\n",
    "\n",
    "                # length of substring 1 is added to get string from next character\n",
    "                res_response = result[0: idx_response].strip()\n",
    "                res_problems = result[idx1_problems + len(sub1_problems) + 1:idx2_problems].strip()\n",
    "                res_corrector = result[idx_corrector + len(sub_corrector) + 1:].strip()\n",
    "                res_question = question[idx1_question + len(sub1_question) + 1:idx2_question].strip()\n",
    "\n",
    "                # adding extracted strings to their respective lists\n",
    "                responses.append(res_response)\n",
    "                problems.append(res_problems)\n",
    "                corrected_responses.append(res_corrector)\n",
    "                questions.append(res_question)\n",
    "                print(\"=====================================================================\")\n",
    "                print(\"End of response for index: \", index_tracker)\n",
    "                print(\"=====================================================================\")\n",
    "            else:\n",
    "                bad_response_indeces.append(index_tracker)\n",
    "                print(\"*******************************************************\")\n",
    "                print(\"Bad response at index: \", index_tracker)\n",
    "                print(\"*******************************************************\")\n",
    "                \n",
    "        else:\n",
    "            none_response_indeces.append(index_tracker)\n",
    "            print(\"*******************************************************\")\n",
    "            print(\"None response at index: \", index_tracker)\n",
    "            print(\"*******************************************************\")\n",
    "\n",
    "        i = i+1\n",
    "        \n",
    "    batch_df = pd.DataFrame(questions, columns=['input'])\n",
    "    batch_df[\"initial_response\"] = responses\n",
    "    batch_df[\"response_problems\"] = problems\n",
    "    batch_df[\"corrected_reponse\"] = corrected_responses\n",
    "    batch_df.to_csv('./data/generated_data'+str(j)+'_'+str(index_tracker)+'.csv', index=False)\n",
    "    \n",
    "    if len(bad_response_indeces)>0:\n",
    "        bad_df = pd.DataFrame(bad_response_indeces, columns=['bad_response_index'])\n",
    "        bad_df.to_csv('./data/bad_responses/bad'+str(j)+'.csv', index=False)\n",
    "    if len(none_response_indeces)>0:\n",
    "        none_df = pd.DataFrame(none_response_indeces, columns=['none_response_index'])\n",
    "        none_df.to_csv('./data/bad_responses/none'+str(j)+'.csv', index=False)\n",
    "    end_time = time.time()\n",
    "    print(\"#################################################################################################\")\n",
    "    print(\"Generated data batch saved at index: \", index_tracker, \" Time taken: \", end_time-start_time)\n",
    "    print(\"#################################################################################################\")\n",
    "    j = j+1\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
